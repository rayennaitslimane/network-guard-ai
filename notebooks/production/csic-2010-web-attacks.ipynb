{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DistilBERT Fine Tuned on CSIC‑2010 Web Attacks Dataset\n",
    "### Rayen NAIT SLIMANE - Masters Student ISIMA INP\n",
    "\n",
    "**NOTE:** This notebook is designed to run on [Kaggle](kaggle.com) with the [CSIC‑2010 Web Attacks Dataset](https://www.kaggle.com/datasets/ispangler/csic-2010-web-application-attacks). Make sure to enable the P100 GPU (free 30h/week quota) for optimal performance during training.\n",
    "\n",
    "I propose fine-tuning DistilBERT, a high-efficiency transformer model, to deliver an AI-powered SQL Injection (SQLi) detection solution. This project will quickly evaluate the model's performance against a Kaggle dataset to establish the most effective and resource-lean strategy for eliminating SQLi vulnerabilities. The goal is to deploy a cutting-edge, high-accuracy defense faster than traditional methods allow.\n",
    "\n",
    "I observed an increase from 60% accuracy and 50% recall to 95% accuracy and 89% precision by using the following methods:\n",
    "\n",
    "* *Used only 'URL', 'METHOD', 'CONTENT', 'USER-AGENT' as the feature set given they're the most essential ones*\n",
    "* *Modifying the concatenated feature strings to incorporate labels like '[URL]:' which is optimal for NLP*\n",
    "* *Imputed the missing values with column specific values like '[NULL_URL]' maintaining the richness of the dataset*\n",
    "\n",
    "Furthermore fine tuning using Low Rank Adaptation (LoRA) enabled me to iterate rapidly through different parameters in order to converge towards an optimal model faster.\n",
    "\n",
    "## Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T08:16:22.965764Z",
     "iopub.status.busy": "2025-10-18T08:16:22.964921Z",
     "iopub.status.idle": "2025-10-18T08:16:27.200731Z",
     "shell.execute_reply": "2025-10-18T08:16:27.199568Z",
     "shell.execute_reply.started": "2025-10-18T08:16:22.965732Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip -q\n",
    "!pip install transformers datasets accelerate peft evaluate safetensors -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T08:17:26.412510Z",
     "iopub.status.busy": "2025-10-18T08:17:26.411808Z",
     "iopub.status.idle": "2025-10-18T08:17:26.417005Z",
     "shell.execute_reply": "2025-10-18T08:17:26.416254Z",
     "shell.execute_reply.started": "2025-10-18T08:17:26.412486Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import torch\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "NUM_LABELS = 2             # change to your task\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 3\n",
    "LR = 2e-5\n",
    "OUTPUT_DIR = \"./lora-distilbert-checkpoint\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T08:17:30.070638Z",
     "iopub.status.busy": "2025-10-18T08:17:30.069856Z",
     "iopub.status.idle": "2025-10-18T08:17:31.001551Z",
     "shell.execute_reply": "2025-10-18T08:17:31.000750Z",
     "shell.execute_reply.started": "2025-10-18T08:17:30.070605Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 61065 entries, 0 to 61064\n",
      "Data columns (total 17 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   Unnamed: 0       61065 non-null  object\n",
      " 1   Method           61065 non-null  object\n",
      " 2   User-Agent       61065 non-null  object\n",
      " 3   Pragma           61065 non-null  object\n",
      " 4   Cache-Control    61065 non-null  object\n",
      " 5   Accept           60668 non-null  object\n",
      " 6   Accept-encoding  61065 non-null  object\n",
      " 7   Accept-charset   61065 non-null  object\n",
      " 8   language         61065 non-null  object\n",
      " 9   host             61065 non-null  object\n",
      " 10  cookie           61065 non-null  object\n",
      " 11  content-type     17977 non-null  object\n",
      " 12  connection       61065 non-null  object\n",
      " 13  lenght           17977 non-null  object\n",
      " 14  content          17977 non-null  object\n",
      " 15  classification   61065 non-null  int64 \n",
      " 16  URL              61065 non-null  object\n",
      "dtypes: int64(1), object(16)\n",
      "memory usage: 7.9+ MB\n",
      "\n",
      " classification\n",
      "0    36000\n",
      "1    25065\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Missing value counts:\n",
      " Unnamed: 0             0\n",
      "Method                 0\n",
      "User-Agent             0\n",
      "Pragma                 0\n",
      "Cache-Control          0\n",
      "Accept               397\n",
      "Accept-encoding        0\n",
      "Accept-charset         0\n",
      "language               0\n",
      "host                   0\n",
      "cookie                 0\n",
      "content-type       43088\n",
      "connection             0\n",
      "lenght             43088\n",
      "content            43088\n",
      "classification         0\n",
      "URL                    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "raw_df = pd.read_csv(\"/kaggle/input/csic-2010-web-application-attacks/csic_database.csv\")\n",
    "raw_df.info()\n",
    "\n",
    "print(\"\\n\", raw_df[\"classification\"].value_counts())\n",
    "print(\"\\nMissing value counts:\\n\", raw_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T08:17:36.713694Z",
     "iopub.status.busy": "2025-10-18T08:17:36.713039Z",
     "iopub.status.idle": "2025-10-18T08:17:37.358060Z",
     "shell.execute_reply": "2025-10-18T08:17:37.357348Z",
     "shell.execute_reply.started": "2025-10-18T08:17:36.713669Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 61065 entries, 0 to 61064\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    61065 non-null  object\n",
      " 1   labels  61065 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 954.3+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[URL]: http://localhost:8080/tienda1/index.jsp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[URL]: http://localhost:8080/tienda1/publico/a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[URL]: http://localhost:8080/tienda1/publico/a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[URL]: http://localhost:8080/tienda1/publico/a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[URL]: http://localhost:8080/tienda1/publico/a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  labels\n",
       "0  [URL]: http://localhost:8080/tienda1/index.jsp...       0\n",
       "1  [URL]: http://localhost:8080/tienda1/publico/a...       0\n",
       "2  [URL]: http://localhost:8080/tienda1/publico/a...       0\n",
       "3  [URL]: http://localhost:8080/tienda1/publico/a...       0\n",
       "4  [URL]: http://localhost:8080/tienda1/publico/a...       0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df = raw_df.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "kept_columns = ['URL', 'Method', 'content', 'User-Agent']\n",
    "kept_columns_null = ['NULL_URL', 'NULL_METHOD', 'NULL_CONTENT', 'NULL_USER_AGENT']\n",
    "\n",
    "for col, null_val in zip(kept_columns, kept_columns_null):\n",
    "    raw_df[col] = raw_df[col].fillna(null_val)\n",
    "    raw_df[col] = raw_df[col].astype(str)\n",
    "\n",
    "raw_df['text_input'] = raw_df.apply(\n",
    "    lambda row: f\"[URL]: {row['URL']} [METHOD]: {row['Method']} [CONTENT]: {row['content']} [USER_AGENT]: {row['User-Agent']}\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'text': raw_df['text_input'],\n",
    "    'labels': raw_df['classification']\n",
    "})\n",
    "\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T08:17:46.163540Z",
     "iopub.status.busy": "2025-10-18T08:17:46.162962Z",
     "iopub.status.idle": "2025-10-18T08:17:46.328818Z",
     "shell.execute_reply": "2025-10-18T08:17:46.327986Z",
     "shell.execute_reply.started": "2025-10-18T08:17:46.163517Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# 1. First split: 80% train+val, 20% test\n",
    "# Note: Use 'Label' as per your DataFrame structure\n",
    "train_val_df, test_df = train_test_split(\n",
    "    df, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=df['labels'] \n",
    ")\n",
    "\n",
    "# 2. Second split: 80% train, 20% validation (of the remaining train_val set)\n",
    "train_df, val_df = train_test_split(\n",
    "    train_val_df, \n",
    "    test_size=0.2, # This is 20% of the 80% (i.e., 16% of the original data)\n",
    "    random_state=42, \n",
    "    stratify=train_val_df['labels']\n",
    ")\n",
    "\n",
    "# 3. Create the DatasetDict object\n",
    "raw_datasets = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(train_df, preserve_index=False),\n",
    "    \"validation\": Dataset.from_pandas(val_df, preserve_index=False),\n",
    "    \"test\": Dataset.from_pandas(test_df, preserve_index=False),\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T08:17:50.434162Z",
     "iopub.status.busy": "2025-10-18T08:17:50.433855Z",
     "iopub.status.idle": "2025-10-18T08:18:03.773329Z",
     "shell.execute_reply": "2025-10-18T08:18:03.772237Z",
     "shell.execute_reply.started": "2025-10-18T08:17:50.434140Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6770f2085e1d4aefadfc4a9a92deac53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d83d04e89aea4e5282b195d9c2857190",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98136cfddc2741259b4cc1b6b4e716ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a645f9caa81747018a0b05d1a78636af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0829ef802af4be1821d786945be1cac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/39081 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7291cbf5aa7444c3910c488d8c560c00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9771 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9cd756f19884ec9a596fc331c110a21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12213 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "def preprocess(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "encoded = raw_datasets.map(preprocess, batched=True)\n",
    "encoded.set_format(\n",
    "    type=\"torch\",\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T08:18:37.008703Z",
     "iopub.status.busy": "2025-10-18T08:18:37.007968Z",
     "iopub.status.idle": "2025-10-18T08:18:37.159368Z",
     "shell.execute_reply": "2025-10-18T08:18:37.158587Z",
     "shell.execute_reply.started": "2025-10-18T08:18:37.008669Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=NUM_LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This LoRA setup balances **efficiency and adaptability**, enabling fast, resource-light fine-tuning with a dropout rate of 0.05 to **reduce overfitting** on the sequence classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T08:18:52.251273Z",
     "iopub.status.busy": "2025-10-18T08:18:52.250989Z",
     "iopub.status.idle": "2025-10-18T08:18:52.255281Z",
     "shell.execute_reply": "2025-10-18T08:18:52.254508Z",
     "shell.execute_reply.started": "2025-10-18T08:18:52.251253Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,               # rank\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_lin\", \"k_lin\", \"v_lin\", \"out_lin\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T08:18:57.933738Z",
     "iopub.status.busy": "2025-10-18T08:18:57.933083Z",
     "iopub.status.idle": "2025-10-18T08:18:57.979178Z",
     "shell.execute_reply": "2025-10-18T08:18:57.978406Z",
     "shell.execute_reply.started": "2025-10-18T08:18:57.933709Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T08:19:00.886065Z",
     "iopub.status.busy": "2025-10-18T08:19:00.885764Z",
     "iopub.status.idle": "2025-10-18T08:19:00.891476Z",
     "shell.execute_reply": "2025-10-18T08:19:00.890628Z",
     "shell.execute_reply.started": "2025-10-18T08:19:00.886045Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = (predictions == labels).mean()\n",
    "    \n",
    "    # Calculate precision, recall, and F1\n",
    "    from sklearn.metrics import precision_recall_fscore_support\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average='binary'\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T08:19:06.860792Z",
     "iopub.status.busy": "2025-10-18T08:19:06.860205Z",
     "iopub.status.idle": "2025-10-18T08:19:06.890119Z",
     "shell.execute_reply": "2025-10-18T08:19:06.889355Z",
     "shell.execute_reply.started": "2025-10-18T08:19:06.860769Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=EPOCHS,\n",
    "    learning_rate=LR,\n",
    "    fp16=torch.cuda.is_available(),   # use mixed precision if GPU available\n",
    "    load_best_model_at_end=False,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T08:19:14.452440Z",
     "iopub.status.busy": "2025-10-18T08:19:14.452117Z",
     "iopub.status.idle": "2025-10-18T08:19:14.743103Z",
     "shell.execute_reply": "2025-10-18T08:19:14.742344Z",
     "shell.execute_reply.started": "2025-10-18T08:19:14.452419Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_37/1937916679.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded[\"train\"],\n",
    "    eval_dataset=encoded[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T08:19:20.498394Z",
     "iopub.status.busy": "2025-10-18T08:19:20.497995Z",
     "iopub.status.idle": "2025-10-18T08:29:39.858161Z",
     "shell.execute_reply": "2025-10-18T08:29:39.857420Z",
     "shell.execute_reply.started": "2025-10-18T08:19:20.498366Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7329' max='7329' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7329/7329 10:17, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.120400</td>\n",
       "      <td>0.135368</td>\n",
       "      <td>0.938184</td>\n",
       "      <td>0.972800</td>\n",
       "      <td>0.873847</td>\n",
       "      <td>0.920672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.121800</td>\n",
       "      <td>0.123291</td>\n",
       "      <td>0.947191</td>\n",
       "      <td>0.982868</td>\n",
       "      <td>0.886811</td>\n",
       "      <td>0.932372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.138300</td>\n",
       "      <td>0.113578</td>\n",
       "      <td>0.950568</td>\n",
       "      <td>0.981441</td>\n",
       "      <td>0.896535</td>\n",
       "      <td>0.937068</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T08:31:01.240234Z",
     "iopub.status.busy": "2025-10-18T08:31:01.239696Z",
     "iopub.status.idle": "2025-10-18T08:32:07.092585Z",
     "shell.execute_reply": "2025-10-18T08:32:07.091917Z",
     "shell.execute_reply.started": "2025-10-18T08:31:01.240210Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Results:\n",
      "eval_loss: 0.1136\n",
      "eval_accuracy: 0.9506\n",
      "eval_precision: 0.9814\n",
      "eval_recall: 0.8965\n",
      "eval_f1: 0.9371\n",
      "eval_runtime: 20.1256\n",
      "eval_samples_per_second: 485.5020\n",
      "eval_steps_per_second: 30.3590\n",
      "epoch: 3.0000\n",
      "\n",
      "Validation dataset size: 9771\n",
      "Validation dataset columns: ['text', 'labels', 'input_ids', 'attention_mask']\n",
      "\n",
      "Predictions shape: (9771, 2)\n",
      "Label IDs: (9771,)\n",
      "Metrics returned: {'test_loss': 0.11357788741588593, 'test_accuracy': 0.9505680073687442, 'test_precision': 0.9814410480349345, 'test_recall': 0.8965345300423835, 'test_f1': 0.9370684039087948, 'test_runtime': 20.5966, 'test_samples_per_second': 474.398, 'test_steps_per_second': 29.665}\n",
      "Number of predictions: 9771\n",
      "Using label_ids from predictions: 9771 labels\n",
      "\n",
      "✓ Final lengths - Predictions: 9771, True labels: 9771\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign       0.93      0.99      0.96      5760\n",
      "   Malicious       0.98      0.90      0.94      4011\n",
      "\n",
      "    accuracy                           0.95      9771\n",
      "   macro avg       0.96      0.94      0.95      9771\n",
      "weighted avg       0.95      0.95      0.95      9771\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[5692   68]\n",
      " [ 415 3596]]\n",
      "\n",
      "[TN  FP]\n",
      "[FN  TP]\n",
      "\n",
      "\n",
      "Test dataset size: 12213\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test predictions: 12213\n",
      "\n",
      "Test Set Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign       0.93      0.99      0.96      7200\n",
      "   Malicious       0.98      0.89      0.94      5013\n",
      "\n",
      "    accuracy                           0.95     12213\n",
      "   macro avg       0.96      0.94      0.95     12213\n",
      "weighted avg       0.95      0.95      0.95     12213\n",
      "\n",
      "\n",
      "Test Confusion Matrix:\n",
      "[[7126   74]\n",
      " [ 533 4480]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "def get_labels_from_logits(logits):\n",
    "    \"\"\"\n",
    "    Convert logits to predicted class labels.\n",
    "    \"\"\"\n",
    "    if isinstance(logits, tuple):\n",
    "        logits = logits[0]\n",
    "    return np.argmax(logits, axis=1)\n",
    "\n",
    "# --- Evaluate on validation set ---\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"\\nValidation Results:\")\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "# --- Detailed validation diagnostics ---\n",
    "print(f\"\\nValidation dataset size: {len(encoded['validation'])}\")\n",
    "print(f\"Validation dataset columns: {encoded['validation'].column_names}\")\n",
    "\n",
    "val_preds = trainer.predict(encoded[\"validation\"])\n",
    "\n",
    "# Debug: Check what we got back\n",
    "print(f\"\\nPredictions shape: {val_preds.predictions.shape}\")\n",
    "print(f\"Label IDs: {val_preds.label_ids.shape if val_preds.label_ids is not None else 'None'}\")\n",
    "print(f\"Metrics returned: {val_preds.metrics}\")\n",
    "\n",
    "val_labels = get_labels_from_logits(val_preds.predictions)\n",
    "print(f\"Number of predictions: {len(val_labels)}\")\n",
    "\n",
    "# CRITICAL: Check if we got all predictions\n",
    "if len(val_labels) != len(encoded[\"validation\"]):\n",
    "    print(f\"\\n⚠️ WARNING: Prediction count mismatch!\")\n",
    "    print(f\"Expected {len(encoded['validation'])} but got {len(val_labels)}\")\n",
    "    print(f\"This suggests the Trainer is dropping samples!\")\n",
    "\n",
    "# Use label_ids if available, otherwise fall back to dataset labels\n",
    "if val_preds.label_ids is not None:\n",
    "    val_true = val_preds.label_ids\n",
    "    print(f\"Using label_ids from predictions: {len(val_true)} labels\")\n",
    "else:\n",
    "    print(\"⚠️ label_ids is None, using fallback\")\n",
    "    val_true = np.array(encoded[\"validation\"][\"labels\"])[:len(val_labels)]\n",
    "    print(f\"Using sliced dataset labels: {len(val_true)} labels\")\n",
    "\n",
    "print(f\"\\n✓ Final lengths - Predictions: {len(val_labels)}, True labels: {len(val_true)}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(val_true, val_labels, target_names=[\"Benign\", \"Malicious\"]))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(val_true, val_labels)\n",
    "print(cm)\n",
    "print(\"\\n[TN  FP]\\n[FN  TP]\")\n",
    "\n",
    "# --- Test set ---\n",
    "if \"test\" in encoded and len(encoded[\"test\"]) > 0:\n",
    "    print(f\"\\n\\nTest dataset size: {len(encoded['test'])}\")\n",
    "    test_preds = trainer.predict(encoded[\"test\"])\n",
    "    test_labels = get_labels_from_logits(test_preds.predictions)\n",
    "    \n",
    "    print(f\"Test predictions: {len(test_labels)}\")\n",
    "    \n",
    "    if test_preds.label_ids is not None:\n",
    "        test_true = test_preds.label_ids\n",
    "    else:\n",
    "        test_true = np.array(encoded[\"test\"][\"labels\"])[:len(test_labels)]\n",
    "    \n",
    "    print(f\"\\nTest Set Results:\")\n",
    "    print(classification_report(test_true, test_labels, target_names=[\"Benign\", \"Malicious\"]))\n",
    "    \n",
    "    print(\"\\nTest Confusion Matrix:\")\n",
    "    print(confusion_matrix(test_true, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T08:43:59.997210Z",
     "iopub.status.busy": "2025-10-18T08:43:59.996489Z",
     "iopub.status.idle": "2025-10-18T08:44:00.308660Z",
     "shell.execute_reply": "2025-10-18T08:44:00.307821Z",
     "shell.execute_reply.started": "2025-10-18T08:43:59.997188Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved LoRA adapter -> ./lora-distilbert-checkpoint\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(\"./lora-distilbert\")\n",
    "print(\"Saved LoRA adapter ->\", OUTPUT_DIR)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 378374,
     "sourceId": 734120,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
